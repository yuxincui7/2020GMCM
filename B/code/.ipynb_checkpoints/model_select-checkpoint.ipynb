{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw_S_content</th>\n",
       "      <th>Raw_RON</th>\n",
       "      <th>Raw_Saturated_hydrocarbon</th>\n",
       "      <th>Raw_Olefins</th>\n",
       "      <th>Raw_Aromatics</th>\n",
       "      <th>Raw_Bromine_value</th>\n",
       "      <th>Raw_Density</th>\n",
       "      <th>Product_S_content</th>\n",
       "      <th>Spent_C</th>\n",
       "      <th>Spent_S</th>\n",
       "      <th>...</th>\n",
       "      <th>S-ZORB.CAL.LEVEL.PV</th>\n",
       "      <th>S-ZORB.FT_1006.DACA.PV</th>\n",
       "      <th>S-ZORB.FT_5204.DACA.PV</th>\n",
       "      <th>S-ZORB.FT_1006.TOTALIZERA.PV</th>\n",
       "      <th>S-ZORB.FT_5204.TOTALIZERA.PV</th>\n",
       "      <th>S-ZORB.FT_1503.DACA.PV</th>\n",
       "      <th>S-ZORB.FT_1503.TOTALIZERA.PV</th>\n",
       "      <th>S-ZORB.FT_1504.DACA.PV</th>\n",
       "      <th>S-ZORB.FT_1504.TOTALIZERA.PV</th>\n",
       "      <th>S-ZORB.PC_1001A.PV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188.0</td>\n",
       "      <td>90.6</td>\n",
       "      <td>53.230000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>22.370000</td>\n",
       "      <td>61.487143</td>\n",
       "      <td>726.085714</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.32</td>\n",
       "      <td>7.30</td>\n",
       "      <td>...</td>\n",
       "      <td>76.849853</td>\n",
       "      <td>6368.74700</td>\n",
       "      <td>233.310805</td>\n",
       "      <td>83086802.0</td>\n",
       "      <td>832503.795</td>\n",
       "      <td>2216.40935</td>\n",
       "      <td>39063124.5</td>\n",
       "      <td>1840.14470</td>\n",
       "      <td>39608757.0</td>\n",
       "      <td>0.353271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>52.300000</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>21.300000</td>\n",
       "      <td>61.880000</td>\n",
       "      <td>731.300000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.37</td>\n",
       "      <td>7.34</td>\n",
       "      <td>...</td>\n",
       "      <td>76.184425</td>\n",
       "      <td>6360.64530</td>\n",
       "      <td>242.369205</td>\n",
       "      <td>82318954.0</td>\n",
       "      <td>803462.665</td>\n",
       "      <td>2370.58740</td>\n",
       "      <td>38810581.5</td>\n",
       "      <td>1641.73260</td>\n",
       "      <td>39389299.0</td>\n",
       "      <td>0.354504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177.0</td>\n",
       "      <td>90.7</td>\n",
       "      <td>52.300000</td>\n",
       "      <td>26.314286</td>\n",
       "      <td>21.385714</td>\n",
       "      <td>61.722857</td>\n",
       "      <td>729.614286</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.43</td>\n",
       "      <td>7.27</td>\n",
       "      <td>...</td>\n",
       "      <td>75.966841</td>\n",
       "      <td>6504.96490</td>\n",
       "      <td>233.076925</td>\n",
       "      <td>82012004.0</td>\n",
       "      <td>791925.055</td>\n",
       "      <td>2326.46535</td>\n",
       "      <td>38693812.0</td>\n",
       "      <td>1600.67575</td>\n",
       "      <td>39312616.5</td>\n",
       "      <td>0.350181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159.0</td>\n",
       "      <td>90.4</td>\n",
       "      <td>52.300000</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>61.330000</td>\n",
       "      <td>725.400000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.08</td>\n",
       "      <td>7.35</td>\n",
       "      <td>...</td>\n",
       "      <td>76.790042</td>\n",
       "      <td>6506.82495</td>\n",
       "      <td>238.349935</td>\n",
       "      <td>81231373.5</td>\n",
       "      <td>762863.810</td>\n",
       "      <td>2495.22360</td>\n",
       "      <td>38410862.5</td>\n",
       "      <td>1563.71215</td>\n",
       "      <td>39120204.5</td>\n",
       "      <td>0.353930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173.0</td>\n",
       "      <td>89.6</td>\n",
       "      <td>52.242857</td>\n",
       "      <td>26.671429</td>\n",
       "      <td>21.085714</td>\n",
       "      <td>61.332857</td>\n",
       "      <td>725.428571</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.45</td>\n",
       "      <td>6.58</td>\n",
       "      <td>...</td>\n",
       "      <td>76.539556</td>\n",
       "      <td>6560.24225</td>\n",
       "      <td>236.576220</td>\n",
       "      <td>80915707.5</td>\n",
       "      <td>751362.300</td>\n",
       "      <td>2807.78910</td>\n",
       "      <td>38283000.0</td>\n",
       "      <td>1554.35740</td>\n",
       "      <td>39045953.5</td>\n",
       "      <td>0.358053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 291 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Raw_S_content  Raw_RON  Raw_Saturated_hydrocarbon  Raw_Olefins  \\\n",
       "0          188.0     90.6                  53.230000    24.400000   \n",
       "1          169.0     90.5                  52.300000    26.400000   \n",
       "2          177.0     90.7                  52.300000    26.314286   \n",
       "3          159.0     90.4                  52.300000    26.100000   \n",
       "4          173.0     89.6                  52.242857    26.671429   \n",
       "\n",
       "   Raw_Aromatics  Raw_Bromine_value  Raw_Density  Product_S_content  Spent_C  \\\n",
       "0      22.370000          61.487143   726.085714                3.2     2.32   \n",
       "1      21.300000          61.880000   731.300000                3.2     2.37   \n",
       "2      21.385714          61.722857   729.614286                3.2     2.43   \n",
       "3      21.600000          61.330000   725.400000                3.2     3.08   \n",
       "4      21.085714          61.332857   725.428571                3.2     2.45   \n",
       "\n",
       "   Spent_S  ...  S-ZORB.CAL.LEVEL.PV  S-ZORB.FT_1006.DACA.PV  \\\n",
       "0     7.30  ...            76.849853              6368.74700   \n",
       "1     7.34  ...            76.184425              6360.64530   \n",
       "2     7.27  ...            75.966841              6504.96490   \n",
       "3     7.35  ...            76.790042              6506.82495   \n",
       "4     6.58  ...            76.539556              6560.24225   \n",
       "\n",
       "   S-ZORB.FT_5204.DACA.PV  S-ZORB.FT_1006.TOTALIZERA.PV  \\\n",
       "0              233.310805                    83086802.0   \n",
       "1              242.369205                    82318954.0   \n",
       "2              233.076925                    82012004.0   \n",
       "3              238.349935                    81231373.5   \n",
       "4              236.576220                    80915707.5   \n",
       "\n",
       "   S-ZORB.FT_5204.TOTALIZERA.PV  S-ZORB.FT_1503.DACA.PV  \\\n",
       "0                    832503.795              2216.40935   \n",
       "1                    803462.665              2370.58740   \n",
       "2                    791925.055              2326.46535   \n",
       "3                    762863.810              2495.22360   \n",
       "4                    751362.300              2807.78910   \n",
       "\n",
       "   S-ZORB.FT_1503.TOTALIZERA.PV  S-ZORB.FT_1504.DACA.PV  \\\n",
       "0                    39063124.5              1840.14470   \n",
       "1                    38810581.5              1641.73260   \n",
       "2                    38693812.0              1600.67575   \n",
       "3                    38410862.5              1563.71215   \n",
       "4                    38283000.0              1554.35740   \n",
       "\n",
       "   S-ZORB.FT_1504.TOTALIZERA.PV  S-ZORB.PC_1001A.PV  \n",
       "0                    39608757.0            0.353271  \n",
       "1                    39389299.0            0.354504  \n",
       "2                    39312616.5            0.350181  \n",
       "3                    39120204.5            0.353930  \n",
       "4                    39045953.5            0.358053  \n",
       "\n",
       "[5 rows x 291 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "table_path = \"../data/data_num_325_reduce.xlsx\"\n",
    "#table_path = \"../data/data_num_325_fea_291.xlsx\"\n",
    "data = pd.read_excel(table_path,index_col = 0)\n",
    "\n",
    "y = data[\"Product_RON_loss\"]\n",
    "X = data.drop(\"Product_RON_loss\",1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into separate training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0787170287004406, tolerance: 0.0010188586153846138\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9931464438975501, tolerance: 0.0010961184615384592\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9715953163666734, tolerance: 0.0011843050384615342\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9366906336505254, tolerance: 0.0010945459615384577\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/yuxinc/.conda/envs/tf22_py38/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2482206229748432, tolerance: 0.0011208453846153802\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#Try common machine learning\n",
    "# R2 Score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def lets_try(train,labels):\n",
    "    results={}\n",
    "    def test_model(clf):\n",
    "        \n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "        r2 = make_scorer(r2_score)\n",
    "        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n",
    "        scores=[r2_val_score.mean()]\n",
    "        return scores\n",
    "\n",
    "    clf = linear_model.LinearRegression()\n",
    "    results[\"Linear\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Ridge()\n",
    "    results[\"Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.BayesianRidge()\n",
    "    results[\"Bayesian Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.HuberRegressor()\n",
    "    results[\"Hubber\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=1e-4)\n",
    "    results[\"Lasso\"]=test_model(clf)\n",
    "    \n",
    "    clf = BaggingRegressor()\n",
    "    results[\"Bagging\"]=test_model(clf)\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    results[\"RandomForest\"]=test_model(clf)\n",
    "    \n",
    "    clf = AdaBoostRegressor()\n",
    "    results[\"AdaBoost\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR()\n",
    "    results[\"SVM RBF\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR(kernel=\"linear\")\n",
    "    results[\"SVM Linear\"]=test_model(clf)\n",
    "    \n",
    "    results = pd.DataFrame.from_dict(results,orient='index')\n",
    "    results.columns=[\"R Square Score\"] \n",
    "    results=results.sort_values(by=[\"R Square Score\"],ascending=False)\n",
    "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.5,1])\n",
    "    return results\n",
    "\n",
    "lets_try(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try auto-sklearn\n",
    "\n",
    "import autosklearn.regression\n",
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=120,\n",
    "    per_run_time_limit=30,\n",
    "    tmp_folder='/tmp/autosklearn_regression_example_tmp',\n",
    "    output_folder='/tmp/autosklearn_regression_example_out',\n",
    ")\n",
    "automl.fit(X_train, y_train, dataset_name='boston')\n",
    "print(automl.show_models())\n",
    "predictions = automl.predict(X_test)\n",
    "print(\"R2 score:\", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try Simple Neural Network\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "# Shape the labels\n",
    "labels_nl = labels\n",
    "labels_nl = labels_nl.reshape(-1,1)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "r2 = tflearn.R2()\n",
    "net = tflearn.input_data(shape=[None, train.shape[1]])\n",
    "net = tflearn.fully_connected(net, 30, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.01, decay_step=100)\n",
    "net = tflearn.regression(net, optimizer=sgd,loss='mean_square',metric=r2)\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(train, labels_nl,show_metric=True,validation_set=0.2,shuffle=True,n_epoch=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf22_py38",
   "language": "python",
   "name": "tf22_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
